# main.py
import json
import streamlit as st
from llm_interface import parse_task_description, generate_pipeline_code
from pipeline_executor import run_pipeline, collect_metrics
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

st.set_page_config(page_title="AutoML LLM App", layout="wide")
st.title("ü§ñ AutoML Pipeline Generator with LLM")

# --- Input from user ---
user_prompt = st.text_area("–û–ø–∏—à—ñ—Ç—å –≤–∞—à—É –∑–∞–¥–∞—á—É (–Ω–∞–ø—Ä–∏–∫–ª–∞–¥, '–∫–ª–∞—Å–∏—Ñ—ñ–∫–∞—Ü—ñ—è –≤—ñ–¥–≥—É–∫—ñ–≤ –∫–ª—ñ—î–Ω—Ç—ñ–≤'):")
uploaded_file = st.file_uploader("–ó–∞–≤–∞–Ω—Ç–∞–∂—Ç–µ CSV-—Ñ–∞–π–ª –∑ –¥–∞–Ω–∏–º–∏:", type=["csv"])
time_limit = st.number_input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∏–π —á–∞—Å –Ω–∞ AutoML (—Å–µ–∫):", min_value=60, value=300, step=60)

if st.button("üîß –ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ —Ç–∞ –∑–∞–ø—É—Å—Ç–∏—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω") and user_prompt and uploaded_file:
    # --- Step 1: Parse prompt to JSON schema ---
    task_schema = parse_task_description(user_prompt)
    st.subheader("üîç –Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–æ–≤–∞–Ω–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –∑–∞–¥–∞—á—ñ")
    st.json(task_schema)

    # --- Step 2: Generate AutoML code ---
    pipeline_code = generate_pipeline_code(task_schema)
    st.subheader("üíª –ó–≥–µ–Ω–µ—Ä–æ–≤–∞–Ω–∏–π –∫–æ–¥ –ø–∞–π–ø–ª–∞–π–Ω—É")
    st.code(pipeline_code, language="python")

    # --- Step 3: Save uploaded CSV ---
    with open("input_data.csv", "wb") as f:
        f.write(uploaded_file.getbuffer())

    df = pd.read_csv("input_data.csv")

    # --- Step 3.0: EDA ---
    st.subheader("üìÑ –û–ø–∏—Å–æ–≤–∞ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞")
    st.write("### –¢–∏–ø–∏ –∑–º—ñ–Ω–Ω–∏—Ö")
    st.write(df.dtypes)

    st.write("### –û—Å–Ω–æ–≤–Ω—ñ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏")
    st.dataframe(df.describe(include='all'))

    st.write("### –ö—ñ–ª—å–∫—ñ—Å—Ç—å –ø—Ä–æ–ø—É—â–µ–Ω–∏—Ö –∑–Ω–∞—á–µ–Ω—å")
    st.dataframe(df.isnull().sum())

    # --- Step 3.0.1: Target distribution ---
    st.subheader("üéØ –†–æ–∑–ø–æ–¥—ñ–ª —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó")
    if task_schema["target"] in df.columns:
        plt.figure(figsize=(8, 4))
        sns.histplot(df[task_schema["target"]], kde=True)
        st.pyplot(plt.gcf())
    else:
        st.warning(f"–¶—ñ–ª—å–æ–≤–∞ –∑–º—ñ–Ω–Ω–∞ '{task_schema['target']}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ —É CSV")

    # --- Step 3.0.2: –ö–æ—Ä–µ–ª—è—Ü—ñ—ó —Ç–∞ —á–∞—Å—Ç–æ—Ç–∏ ---
    numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns
    if len(numeric_cols) > 1:
        st.subheader("üîó –ö–æ—Ä–µ–ª—è—Ü—ñ—ó –º—ñ–∂ —á–∏—Å–ª–æ–≤–∏–º–∏ –∑–º—ñ–Ω–Ω–∏–º–∏")
        plt.figure(figsize=(10, 6))
        sns.heatmap(df[numeric_cols].corr(), annot=True, cmap="coolwarm")
        st.pyplot(plt.gcf())

    categorical_cols = df.select_dtypes(include='object').columns
    for col in categorical_cols:
        st.subheader(f"üì¶ –ß–∞—Å—Ç–æ—Ç–∏ –¥–ª—è '{col}'")
        st.bar_chart(df[col].value_counts())

    # --- Step 3.1: Split into train/test ---
    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)
    train_df.to_csv("input_data.csv", index=False)     # –¥–ª—è AutoML
    test_df.to_csv("test_data.csv", index=False)       # –¥–ª—è –º–µ—Ç—Ä–∏–∫

    # --- Step 4: Run AutoML pipeline ---
    st.subheader("üöÄ –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ...")
    model, leaderboard = run_pipeline(pipeline_code, "input_data.csv", time_limit)
    st.write("## üìä –†–µ–∑—É–ª—å—Ç–∞—Ç–∏ AutoML")
    st.dataframe(leaderboard)

    # --- Step 4.1: Verbose model summary ---
    st.subheader("üìå –ù–∞–π–∫—Ä–∞—â–∞ –º–æ–¥–µ–ª—å")
    best_model = leaderboard.iloc[0]
    st.write(f"üîπ –ú–æ–¥–µ–ª—å: `{best_model['model']}`")
    st.write(f"üîπ Score: `{best_model['score_val']:.4f}`")
    st.write(f"üîπ –ß–∞—Å –Ω–∞–≤—á–∞–Ω–Ω—è: `{best_model['fit_time']:.2f}` —Å–µ–∫")

    if task_schema["task_type"] == "classification":
        st.info("‚û°Ô∏è –ß–∏–º –≤–∏—â–∏–π accuracy –∞–±–æ F1, —Ç–∏–º –∫—Ä–∞—â–µ.")
    else:
        st.info("‚û°Ô∏è –ß–∏–º –Ω–∏–∂—á–∏–π RMSE / MAE, —Ç–∏–º –∫—Ä–∞—â–µ.")

    # --- Step 4.2: Feature importance (—è–∫—â–æ –º–æ–∂–ª–∏–≤–æ) ---
    st.subheader("üìà –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å —Ñ—ñ—á (Feature Importance)")
    try:
        test_data = pd.read_csv("test_data.csv")
        fi_df = model.feature_importance(test_data)
        st.dataframe(fi_df.head(10))

        plt.figure(figsize=(10, 6))
        sns.barplot(data=fi_df.head(10), x="importance", y="feature")
        plt.title("Top-10 –≤–∞–∂–ª–∏–≤–∏—Ö —Ñ—ñ—á")
        st.pyplot(plt.gcf())
    except Exception as e:
        st.warning(f"‚ö†Ô∏è –ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –≤–∞–∂–ª–∏–≤—ñ—Å—Ç—å —Ñ—ñ—á: {e}")

    # --- Step 5: Collect metrics ---
    st.subheader("üìâ –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ —Ç–µ—Å—Ç–æ–≤–∏—Ö –¥–∞–Ω–∏—Ö")
    metrics, figs = collect_metrics(model, test_data)
    st.json(metrics)

    for fig in figs:
        st.pyplot(fig)

    st.success("‚úÖ –ü–∞–π–ø–ª–∞–π–Ω —É—Å–ø—ñ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
